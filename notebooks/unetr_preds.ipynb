{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTMAEConfig, ViTMAEForPreTraining, ViTConfig, ViTModel\n",
    "\n",
    "from src.datasets import UnetCustomDataset, unet_train_collate, unet_valid_collate\n",
    "from src.plotters import visualize_predictions\n",
    "from src.unetr_4x4 import CustomUNETR\n",
    "from src.unetr_trainer import UNETR_TRAINER\n",
    "from src.utils import select_device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original VITMAE config (retrieve from logs/info)\n",
    "vitmaeconfig = {\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"decoder_hidden_size\": 192,\n",
    "    \"decoder_intermediate_size\": 768,\n",
    "    \"decoder_num_attention_heads\": 6,\n",
    "    \"decoder_num_hidden_layers\": 6,\n",
    "    \"hidden_act\": \"gelu\",\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"hidden_size\": 192,\n",
    "    \"image_size\": 64,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"intermediate_size\": 768,\n",
    "    \"layer_norm_eps\": 1e-05,\n",
    "    \"mask_ratio\": 0.5,\n",
    "    \"model_type\": \"vit_mae\",\n",
    "    \"norm_pix_loss\": 1,\n",
    "    \"num_attention_heads\": 6,\n",
    "    \"num_channels\": 16,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"patch_size\": 4,\n",
    "    \"qkv_bias\": True,\n",
    "    \"transformers_version\": \"4.42.3\"\n",
    "}\n",
    "\n",
    "# Configuration for VIT encoder for UNETR\n",
    "vitconfig = {\n",
    "    \"hidden_size\": vitmaeconfig[\"hidden_size\"],\n",
    "    \"num_hidden_layers\": vitmaeconfig[\"num_hidden_layers\"],\n",
    "    \"num_attention_heads\": vitmaeconfig[\"num_attention_heads\"],\n",
    "    \"intermediate_size\": vitmaeconfig[\"intermediate_size\"],\n",
    "    \"hidden_act\": vitmaeconfig[\"hidden_act\"],\n",
    "    \"hidden_dropout_prob\": vitmaeconfig[\"hidden_dropout_prob\"],\n",
    "    \"attention_probs_dropout_prob\": vitmaeconfig[\"attention_probs_dropout_prob\"],\n",
    "    \"initializer_range\": vitmaeconfig[\"initializer_range\"],\n",
    "    \"layer_norm_eps\": vitmaeconfig[\"layer_norm_eps\"],\n",
    "    \"image_size\": vitmaeconfig[\"image_size\"],\n",
    "    \"patch_size\": vitmaeconfig[\"patch_size\"],\n",
    "    \"num_channels\": vitmaeconfig[\"num_channels\"],\n",
    "    \"qkv_bias\": vitmaeconfig[\"qkv_bias\"],\n",
    "    \"encoder_stride\": vitmaeconfig[\"patch_size\"],\n",
    "}\n",
    "\n",
    "'''Extracting Pretrained VITMAE Encoder'''\n",
    "pretrained_model_path = \"/home/mhill/Projects/cathepsin/logs/vitmae-grid/02/model.pth\"\n",
    "checkpoint = torch.load(pretrained_model_path)\n",
    "vitmae_model = ViTMAEForPreTraining(config=ViTMAEConfig(**vitmaeconfig))\n",
    "vitmae_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "'''Transfer to new VITCONFIG'''\n",
    "vitmae_encoder = vitmae_model.vit\n",
    "vit = ViTModel(config=ViTConfig(**vitconfig))\n",
    "vit.load_state_dict(vitmae_encoder.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "''' Loading Train Data '''\n",
    "\n",
    "train_data = np.load(\"/home/mhill/Projects/cathepsin/data/unet_training_dataset.npz\")\n",
    "train_images = train_data['images']\n",
    "train_labels = train_data['labels']\n",
    "train_dataset = UnetCustomDataset(train_images, train_labels)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=unet_train_collate\n",
    ")\n",
    "\n",
    "''' Load Valid Data '''\n",
    "\n",
    "valid_data = np.load('/home/mhill/Projects/cathepsin/data/unet_validation_dataset.npz')\n",
    "valid_images = valid_data['images']\n",
    "valid_labels = valid_data['labels']\n",
    "valid_dataset = UnetCustomDataset(valid_images, valid_labels)\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset=valid_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=unet_valid_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_device()\n",
    "unet_model = CustomUNETR(encoder=vit, num_classes=16, feature_size=32).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n",
    "optimizer = torch.optim.Adam(unet_model.parameters(), lr=1e-3)\n",
    "\n",
    "trainer = UNETR_TRAINER(model=unet_model,\n",
    "                        optimizer=optimizer,\n",
    "                        criterion=criterion,\n",
    "                        device='cuda')\n",
    "\n",
    "model = trainer.fit(num_epochs=5,\n",
    "                    train_batches=train_dataloader,\n",
    "                    valid_batches=valid_dataloader,\n",
    "                    train_eval_batches=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.eval()\n",
    "with torch.inference_mode():\n",
    "    images, labels = next(iter(valid_dataloader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images)\n",
    "    predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "    # Convert to numpy arrays for visualization\n",
    "    images_np = images.cpu().numpy()\n",
    "    labels_np = labels.cpu().numpy()\n",
    "    predictions_np = predictions.cpu().numpy()\n",
    "    print(\n",
    "        f\"Shape of image_np : {images_np.shape} | Label_np : {labels_np.shape} | Preidcitons : {predictions_np.shape}\")\n",
    "    # Visualize the predictions\n",
    "    visualize_predictions(5, images_np, labels_np, predictions_np)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
